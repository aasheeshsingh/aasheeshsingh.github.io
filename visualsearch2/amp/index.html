
<head>
    <meta charset="utf-8">

    <title>Part 2 : Cognitive Models for Visual Search</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Aasheesh Singh ">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Part 2 : Cognitive Models for Visual Search">
    <meta property="og:description" content="In Part 1 of this series, we discussed how simulating users behaviour would be the second step in the pipeline of building Adaptive Interfaces that personalise to user's needs. In this post, we would consider one such simulator model for the task of Visual Search in drop down Menus. Let">
    <meta property="og:url" content="http://localhost:2368/visualsearch2/">
    <meta property="og:image" content="http://localhost:2368/content/images/2019/01/selection_time-1.png">
    <meta property="article:published_time" content="2019-01-01T06:22:00.000Z">
    <meta property="article:modified_time" content="2019-01-08T18:59:39.000Z">
    <meta property="article:tag" content="Human Computer Interaction">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="User Interfaces">
    <meta property="article:tag" content="Menu Models">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Part 2 : Cognitive Models for Visual Search">
    <meta name="twitter:description" content="In Part 1 of this series, we discussed how simulating users behaviour would be the second step in the pipeline of building Adaptive Interfaces that personalise to user's needs. In this post, we would consider one such simulator model for the task of Visual Search in drop down Menus. Let">
    <meta name="twitter:url" content="http://localhost:2368/visualsearch2/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2019/01/selection_time-1.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Aasheesh Singh">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Human Computer Interaction, Reinforcement Learning, User Interfaces, Menu Models">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Aasheesh Singh ",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Aasheesh Singh",
        "url": "http://localhost:2368/author/aasheesh/",
        "sameAs": []
    },
    "headline": "Part 2 : Cognitive Models for Visual Search",
    "url": "http://localhost:2368/visualsearch2/",
    "datePublished": "2019-01-01T06:22:00.000Z",
    "dateModified": "2019-01-08T18:59:39.000Z",
    "image": "http://localhost:2368/content/images/2019/01/selection_time-1.png",
    "keywords": "Human Computer Interaction, Reinforcement Learning, User Interfaces, Menu Models",
    "description": "In Part 1 of this series, we discussed how simulating users behaviour would be the second step in the pipeline of building Adaptive Interfaces that personalise to user&#x27;s needs. In this post, we would consider one such simulator model for the task of Visual Search in drop down Menus. Let",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.14">
    <link rel="alternate" type="application/rss+xml" title="Aasheesh Singh " href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">Aasheesh Singh </a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Part 2 : Cognitive Models for Visual Search</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/aasheesh/">Aasheesh Singh</a></p>
                    <time class="post-date" datetime="2019-01-01">2019-01-01</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="http://localhost:2368/content/images/2019/01/selection_time-1.png" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>In <a href="https://ashsingh.me/visualsearch1/">Part 1</a> of this series, we discussed how simulating users behaviour would be the second step in the pipeline of building Adaptive Interfaces that personalise to user's needs. In this post, we would consider one such simulator model for the task of Visual Search in drop down Menus.</p>
<p>Let us start with an example where our goal is to build a UI targeted towards a particular demographic(say, Medicine/Health tracking app for Elderly). One way we could approach this problem is to conduct experiments and use the insights gained (avg response time for selection, visual acuity, readability preference etc) in our design choices(font, color, layout, shortcuts, input methods) to ease usage amongst that demographic. This works well, but is expensive and takes time to conduct individual experiments. To avoid or aid this process, developing simulated models that mimic users strategy have been an active area of research in HCI. Broadly such methods can be divided into three categories:</p>
<ul>
<li>Map based( Salience, activation)</li>
<li>Bayesian MAP based</li>
<li>Optimal Control based</li>
</ul>
<p>In this post our subject of interest would be Optimal Control methods or <strong>Computationally rational models</strong>. They are based on the idea that user behaviour emerges to maximise underlying <em>Utility</em> under cognitive and perceptual limits. Thus the user <em>strategy</em> emerges as an optimal policy and hence it's called a Computationally rational model.<br>
</p>
<p>One such task which can be modeled as a rational behaviour is searching through a Menu to find a target item. Visual Search is something that we do hundreds of times a day and therefore is a necessary step in ultimately solving Visual Intelligence.</p>
<hr>
<p>The idea of using Reinforcement learning to derive computationally rational strategies to solve a Visual Search task was introduced in this work. <a href="http://users.comnet.aalto.fi/oulasvir/pubs/pn1895-pcs-pdf.pdf">Chen X.,Bailly G., Brumby D., Oulasvirta A.,Howes A.<strong>The Emergence of Interactive Behaviour:A Model of Rational Menu Search</strong>,2015</a>. The authors investigate the effect of Menu organization <b>(Alphabetic,Semantic, Unordered)</b> and Menu length on the <em>emergent</em> optimal strategy and compare the model predictions with empirical findings.</p>
<p>Our work on this model is a derivative of the aforementioned work in the following ways:</p>
<ul>
<li>
<p>The model makes the same hypothesis of <u>Optimal control</u> that the authors investigated where the task of Menu Search is rationally adapted to Ecological structure of interaction(what agent observes the environment), Mechanism(how agent observes the environment: Perceptual limits,peripheral vision) and Utility( Maximizing speed and accuracy).</p>
</li>
<li>
<p>The model uses the same structure for Reward and Peripheral vision reported in the work where at each step the Time duration to take that step (fixation,saccade duration) is appended as a negative reward. Upon solving the task, the agent is highly rewarded or punished upon failure.</p>
</li>
</ul>
<p>Our model is differentiated in the following ways:</p>
<ul>
<li>The observation model or the Ecology structure uses a <strong>Partially observed model</strong> where the user observes the <u>Semantic and Shape relevance</u> of the fixated item in the Menu and then samples the probability of it being a target from its internal belief distribution.</li>
</ul>
<p><strong>This allows us to parameterize the internal cognitive distribution of the User and can be used to model the very different behaviours observed in Expert vs Novice user for the same task.</strong></p>
<blockquote>
<p>An Expert user would have a belief distribution closer to the true distribution from which these relevances were sampled. The KL divergence between these two distributions could be used as a parameter to model User Expertise.</p>
</blockquote>
<ul>
<li>Therefore now we can solve the PO-MDP as a continuous state MDP where the state is the user's current belief of the Target being present at that location in the Menu along with an additional belief of Target being absent.</li>
</ul>
<p>With the state space now being continuous and hence having practically infinite states a tabular Q learning method such as the one used in the above paper wouldn't work without some sort of Function approximation. We used a <strong>Gaussian Process</strong> method <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6420&amp;rep=rep1&amp;type=pdf">GP-SARSA</a> to approximate the Q function and make use of the covariance matrix to aid the agent's exploration strategy.</p>
<p>The advantage of using such Bayesian methods over currently popular Deep learning/Gradient based methods is that it allows us to encode the prior information from a HCI research perspective in a structured way. This is essential when we are trying to form a simulator model whose eventual aim is to be able to explain empirical findings.</p>
<p>For a complete study of the RL learner Gaussian Process-SARSA there would be another post in the "Reproducibility in Machine Learning" series where I would reproduce the experiments of the original work on a Maze environment.</p>
<hr>
<h3 id="modeloverview">Model Overview</h3>
<p></p>
<p>The two versions of the GP-SARSA learner were implemented with sparse and non sparse dictionary. In the sparsified version, a threshold (hyperparameter) determines which state transitions or steps would be used as data points to fit our Gaussian prior. This reduces the execution time of the algorithm from O(t<sup>3</sup>) to O(tm<sup>2</sup>) where t=total transitions and m=data points in dictionary. The ratio of 'm' to 't' is controlled by threshold parameter.</p>
<p>To get a starting point regarding the setting of this threshold i plotted the distribution of the values that this threshold controls(delta). </p>
<p>With experiments, i observed that a value of 0.5 works best. A higher value excludes a lot of potentially important transitions and a lower value has repetitive transitions with not much entropy.</p>
<h3 id="policy">Policy</h3>
<p>The learner was trained with the following action selection policies:</p>
<ul>
<li><strong>Epsilon-Greedy:</strong> Standard policy used in RL where value of Epsilon controls the exploitation-exploration trade-off. High value of epsilon leads to more exploration.</li>
<li><strong>Active learning</strong> based on Covariance: Here we use the covariance matrix of the Q function Gaussian Process and explore the actions which have currently high uncertainty in their Q values(Expected payoff). Thus we would take,
<ul>
<li>Action with highest mean with 1-Eps probability</li>
<li>Action with highest uncertainty with Epsilon prob.</li>
</ul>
</li>
</ul>
<p>Since the covariance of these State-Action values update as the agent learns, its called an Active learning approach.</p>
<ul>
<li><strong>Stochastic Learning :</strong> Here a parameter called <strong>Covariance Scale</strong> is used which controls the spread of the Q value distribution. The action selection rule is defined as:<br>
<b>Action(s) = argmax<sub>a</sub>{Q'(s, a):a ∈ Ao}</b></li>
</ul>
<p>where,Q'(s,a) denotes a <strong>sample</strong>: Q'(s, a) ∼ N (Q(s, a),η<sup>2</sup>cov((s, a),(s, a)))</p>
<p>and η<sup>2</sup>=Covariance Scale</p>
<p>As iterated in the original literature on GP-SARSA, this policy provides faster convergence but sometimes leads to a sub-optimal policy. This could be explained from the fact that the way this policy handles exploration is by randomly trying Actions with overlapping distribution. In our case the distribution of Click/Quit Actions varies significantly from Fixation Actions and hence this policy inhibits exploring those actions in initial training and thus leads to a policy where a lot of fixations are done before selection.</p>
<p>With our experiments we observed that <u>Active learning</u> led to a better policy.</p>
<hr>
<h3 id="experiments">Experiments</h3>
<p>In the aforementioned work, authors conducted experiments to measure <strong>Selection time</strong> in the Visual Search task to measure how a learner trained on one type of Menu performs across other menus.(on which it was not trained). The Selection time is made up of the time to fixate on an item and saccade duration. Their model corroborated what they observed in empirical data that Alphabetic menu layout is the most efficient in terms of selection time. In the results below, a comparison between Semantic and Unordered layout is done(Alphabetic to follow). The policy is trained on Semantically organised Menu items and the agent's selection time and Gaze proportion is evaluated on Semantic and Unordered Menus.</p>
<p><strong>Selection Time:</strong><br>
We compare the Selection Times of our learner with the results in the above work.<br>
</p>
<center> Fig.5 Search Duration with 95% CI, Chen et.al,2015</center>
<p>The above plot shows the Selection Time for the visual search task in a 10 item Menu. The agent's performance was recorded before and after training on Alphabetic, Semantic and Unordered Menus. For semantically trained learners, the mean selection time is 2000 ms and <u>avg steps ~ 4.6 </u>(~ 437ms per fixation) before making a selection.</p>
<p>We demonstrate a similar plot for the learner's performance before and after training where it's trained on a Semantically organised Menu and its selection time is compared to its performance on an Unordered Menu(more comprehensive evaluation across menus to follow). For our experiments we used a 8 item Menu grouped in 2 and it took on an <u> avg~ 2.8 steps to successful selection</u> in a Semantic Menu.</p>
<p></p>
<p><strong>Gaze Proportion</strong></p>
<p>Analyzing the gaze distribution of the user when it's interacting with an Interface is a key to building personalised adaptive UIs. We looked in <a href="https://ashsingh.me/visualsearch1/">Part 1</a> of this series how UI designers are interested in knowing about user's behavior(through mouse movements or gaze) when interacting with their product and thus Gaze Analytics as a Service has become an essential tool for them.(<a href="https://heatmap.com/">Heat Map Inc</a> sells such analytics services as Javascript wrappers)</p>
<p>Continuing in similar fashion ,we take a look at the <strong>Gaze proportions</strong> on the Target item while navigating the Menu, first from the <a href="http://users.comnet.aalto.fi/oulasvir/pubs/pn1895-pcs-pdf.pdf">Chen et.al,2015</a> and then our experiments.</p>
<p></p>
<p>Down below, are the plots for <strong>Gaze proportions</strong> in the learned policies trained on Semantic Menus compared with Unordered Menu and Initial untrained learner.</p>
<p></p>
<p>Here we can see that an untrained agent, distributes the gaze evenly among all target items while the trained policy has learnt to focus on specific items in the menu while looking for target.</p>
<p>The reason for such high gaze proportion when the Target is at 3(0 indexed x axis) is because the policy learnt to fixate on Item 3 as the first action to be taken for each episode, <u>as it allows the agent to encode the semantic relevances of both groups in the menu through peripheral vision</u> (encode info of one item above and down from fixation location) and update the belief state.</p>
<p></p>
<hr>
<h3 id="conclusion">Conclusion</h3>
<p>The work in Chen et. al,2015 assumes a <u>fully observable</u> and <u>deterministic</u> Ecology structure where the user knows exactly the relevances of the menu items. This is what our model would formulate as Expert behaviour in terms of observation mechanism. The inclusion of a internal belief distribution whose divergence from this expert true distribution could be used to model the "Familiarity" or "<strong>proficiency</strong>" of the user in this task.</p>
<p>A user who shows a  different mental ability(people with special needs) or cognitive load while doing such tasks would have a different observed behaviour(in terms of selection time/utility) and thus may not fit this model. What our model attempts is to <u>encode such Proficiency as a parameter</u> and also makes it easy to introduce other perceptual signals(apart from semantic and shape) without changing the learner or the environment structure.</p>
<p>In part 1 of this series I have written about how personalization in Streaming,E-commerce and Advertisements has helped us build better products. I believe Interfaces that adapt and change according to the cognitive skills of the user are the next step to building better software.</p>
<p>The other features of this model are <u>scalablity</u> and dealing with observation noise. A tabular Q learner such as one used in the paper suffers from curse of dimensionality as we scale the state-action space, while this continuous model would be better suited to encode further perceptual information and corresponding noise.</p>
<hr>
<blockquote>
<h4 id="inpart3ofthishciseriesnextweekthegoalwouldbetoreportcomprehensivestudiesforthismodelandhowbayesianinferencecouldbeusedforparametertuning">In part 3 of this HCI series next week the goal would be to report comprehensive studies for this model and how Bayesian inference could be used for parameter tuning.</h4>
</blockquote>


            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">Aasheesh Singh </a> © 2019</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
