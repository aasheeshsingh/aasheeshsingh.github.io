<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Reproducible Machine Learning]]></title><description><![CDATA[Aasheesh Singh]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Reproducible Machine Learning</title><link>http://localhost:2368/</link></image><generator>Ghost 2.14</generator><lastBuildDate>Mon, 04 Mar 2019 04:24:16 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Building Adaptive Interfaces: Software 3.0]]></title><description><![CDATA[Andrew Ng predicted the impact Machine Learning would have on the way we develop software.Could we use ML to make User Interactive Models smarter as well? Let's find out!]]></description><link>http://localhost:2368/visualsearch1/</link><guid isPermaLink="false">5c6e706aa3007c27278140b8</guid><category><![CDATA[Human Computer Interaction]]></category><category><![CDATA[Reinforcement Learning]]></category><dc:creator><![CDATA[Aasheesh Singh]]></dc:creator><pubDate>Tue, 01 Jan 2019 10:38:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2019/03/1_dt5fOj7biRR1L91ixI1iew-1.png" medium="image"/><content:encoded><![CDATA[ <img src="http://localhost:2368/content/images/2019/03/1_dt5fOj7biRR1L91ixI1iew-1.png" alt="Building Adaptive Interfaces: Software 3.0"><p style="text-align:justify">
Andrew Ng predicted the impact Machine Learning would have on the way we develop software and where the community could be heading next. Could we use ML to make User Interactive Models smarter as well? Let's find out. 
</p>
<blockquote class="twitter-tweet" data-lang="en-gb"><p lang="en" dir="ltr">1/The rise of Software Engineering required inventing processes like version control, code review, agile, to help teams work effectively. The rise of AI &amp; Machine Learning Engineering is now requiring new processes, like how we split train/dev/test, model zoos, etc.</p>&mdash; Andrew Ng (@AndrewYNg) <a href="https://twitter.com/AndrewYNg/status/1080886439380869122?ref_src=twsrc%5Etfw">3 January 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p style="text-align:justify">
Studying Machine learning has made me quite fond of labeling things and if I try to label how the paradigm of software development has changed over the years and what's ahead i would do it as follows:
</p>
<ol>
<li>
<p><strong>Software 1.0</strong> : Software to solve problems. Search Engine, Websites, OS, Security.</p>
</li>
<li>
<p><strong>Software 2.0</strong> : Software to help build and monitor software. Version Control, Cloud Services, Slack etc.</p>
</li>
<li>
<p><strong>Software 3.0</strong>: Software that writes software. MLAS( Drag and drop Machine Learning as a Service), Adaptive User Interfaces/IDEs.</p>
</li>
</ol>
<p style="text-align:justify">
In this post, I would like to discuss more on <b><u>Adaptive Software</u></b> particularly the ones which are designed to be User Interfaces. Let's think of software with vast number of selection commands like <i>PhotoShop, Adobe Premiere or Illustrator</i>. Designing such interfaces and organising the layouts(drop downs) requires experienced UI Designers and involves lots of R & D man-hours in user studies. Fast forward a couple of million dollars, a few offshore teams and now we have a product that has all the right things: easy readability (through user studies), Hot key shortcuts, hierarchical layouts(alphabetic,semantic etc), clean graphics and hopefully doesn't clog up the RAM. (Looking at you Slack!)
</p><p style="text-align:justify">
Awesome right! but not really, think of this if all I want to do is to compile cat videos on Adobe Premiere or make memes on Photoshop why do I need this spatially optimised multi window 100 commands layout that has been designed considering an experienced user and would make me look at 5 drop downs to do something. Therefore for such products to cover a wider user base an adaptive or personalised interface makes sense. Basically an interface that adapts from what you do. Supervised Machine Learning has opened doors for massive personalised content in Streaming, E-Commerce and Advertisements and Adaptive User Interfaces should be the next obvious candidate. </p>
<p>We could divide the Adaptability task into three levels, each building on top of the other.</p>
<blockquote>
<h3 id="level1providebehavioranalyticsofusersclicktimefocusdurationheatmapofvisualacuitygazelocationsetctoaidinthedesignoftheuimenulayouts">Level 1: Provide behavior analytics of user's click time, focus duration, heat-map of visual acuity, gaze locations etc. to aid in the design of the UI Menu layouts.</h3>
</blockquote>
<p style="text-align:justify">
<a href="https://heatmap.com/"> Heat Map Inc </a> provides such analytics services as a Java Script wrapper to embed in websites and monitor user's mouse click movements.Pair it up with a visual eye tracker and you have yourself a <b> Gaze analytics service to strategically design the layout or even to look for the premium spot for advertising in terms of visual acuity.</b> </p>
<p><img src="http://localhost:2368/content/images/2019/03/1_dt5fOj7biRR1L91ixI1iew.png" alt="Building Adaptive Interfaces: Software 3.0"></p>
<blockquote>
<h3 id="level2targeteddesigns">Level 2: Targeted Designs</h3>
</blockquote>
<p style="text-align:justify">The next step would be to form a cognitive model of the user behavior that could replicate how users would react(response time, visual acuity, focus) to changes in the UI. A simulator that would model different class of users(age,medical condition, motor skills) would allow targeted products for a demographic to be developed quickly(Parameter inference from prior data) without having to conduct user studies each time.</p>
<blockquote>
<h3 id="level3optimaluigenerator">Level 3: Optimal UI Generator</h3>
</blockquote>
<p style="text-align:justify">Alright! so we simulated some users, saw what they do, now what! Well the next step is to move to <b>generative models</b> that would learn the intrinsic distribution(or produce samples from this distribution) between the optimal design choices of a interface. Say Something like Generative Adversarial Network which has shown promise in Vision where through a MinMax type optimization the Generator is able to synthesize samples from the original distribution. </p>
<p>In HCI, this would look something like this:</p>
<ul>
<li>A dataset indicating design choices(font size, color, window size,layout etc).</li>
<li>The Input Parameters being a distribution encompassing various cognitive attributes of the user, some of which are explicitly stated( wears glasses, age) while others(motor skills, cognitive capacity, peripheral vision) not so explicit and would be inferred as the system interacts.</li>
</ul>
<blockquote>
<h4 id="asanexampletheadaptiveinterfacewouldintelligentlydesignverydifferentuisforletssayourtwousersrickandmortyrickwhois38hasgoodvisionisacommandlinegeekandprefersusinghotkeysshortcutswouldhaveverydifferentpersonalisedinterfaceovermortywhois23wearsglassesandismoreefficientwithtouchpadgesturestheadaptabilitythereforereferstobeingabletomaximiseutilityforindividualusersbyobservingtheircognitionmechanismandgeneratingasuitableecologyenvironmentstructuretermsdefinedinpayneetal2013">As an example, the Adaptive interface would intelligently design very different UIs for let's say our two users, Rick and Morty. Rick who is 38, has good vision, is a command line geek and prefers using Hot keys shortcuts would have very different personalised interface over Morty who is 23, wears glasses and is more efficient with touchpad gestures. The Adaptability therefore refers to being able to maximise <strong>Utility</strong> for individual users by observing their cognition <strong>Mechanism</strong> and generating a suitable <strong>Ecology</strong>(environment structure). (Terms defined in Payne et. al,2013)</h4>
</blockquote>
<p>The figure below shows a Generative model that would learn to sample from the distribution of the best design choices parameters.<br>
<img src="http://localhost:2368/content/images/2019/03/1_AGOoq5Xx-98wa2zMyORvYw.jpeg" alt="Building Adaptive Interfaces: Software 3.0"></p>
<hr>
<blockquote>
<h4 id="inpart2ofthisserieswewouldfocusonthelevel2modeliesimulatingtheuserwewouldmodelthetaskofvisuallysearchingatargetiteminthemenuareinforcementlearningagentwouldmodelthecognitivebehaviouroftheusertoderivearationalstrategyofsearchingthetargetitempart2visualsearch">In Part 2 of this series we would focus on the Level 2 Model i.e simulating the user. We would model the task of visually searching a Target item in the Menu. A Reinforcement learning agent would model the cognitive behaviour of the user to derive a rational strategy of searching the target item. <a href="https://aasheeshsingh.me/visualsearch2/">Part 2: Visual Search</a></h4>
</blockquote>
]]></content:encoded></item><item><title><![CDATA[Part 2 : Cognitive Models for Visual Search]]></title><description><![CDATA[How does users performance vary across Menu layouts and could we simulate user behaviour with a Reinforcement learning. Let's find out!]]></description><link>http://localhost:2368/visualsearch2/</link><guid isPermaLink="false">5c6e706aa3007c27278140b9</guid><category><![CDATA[Human Computer Interaction]]></category><category><![CDATA[Reinforcement Learning]]></category><category><![CDATA[User Interfaces]]></category><category><![CDATA[Menu Models]]></category><dc:creator><![CDATA[Aasheesh Singh]]></dc:creator><pubDate>Tue, 01 Jan 2019 06:22:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2019/03/selection_time-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2019/03/selection_time-1.png" alt="Part 2 : Cognitive Models for Visual Search"><p>In <a href="https://aasheeshsingh.me/visualsearch1/">Part 1</a> of this series, we discussed how simulating users behaviour would be the second step in the pipeline of building Adaptive Interfaces that personalise to user's needs. In this post, we would consider one such simulator model for the task of Visual Search in drop down Menus.</p>
<p>Let us start with an example where our goal is to build a UI targeted towards a particular demographic(say, Medicine/Health tracking app for Elderly). One way we could approach this problem is to conduct experiments and use the insights gained (avg response time for selection, visual acuity, readability preference etc) in our design choices(font, color, layout, shortcuts, input methods) to ease usage amongst that demographic. This works well, but is expensive and takes time to conduct individual experiments. To avoid or aid this process, developing simulated models that mimic users strategy have been an active area of research in HCI. Broadly such methods can be divided into three categories:</p>
<ul>
<li>Map based( Salience, activation)</li>
<li>Bayesian MAP based</li>
<li>Optimal Control based</li>
</ul>
<p>In this post our subject of interest would be Optimal Control methods or <strong>Computationally rational models</strong>. They are based on the idea that user behaviour emerges to maximise underlying <em>Utility</em> under cognitive and perceptual limits. Thus the user <em>strategy</em> emerges as an optimal policy and hence it's called a Computationally rational model.<br>
<img src="http://localhost:2368/content/images/2019/03/edit1.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p>One such task which can be modeled as a rational behaviour is searching through a Menu to find a target item. Visual Search is something that we do hundreds of times a day and therefore is a necessary step in ultimately solving Visual Intelligence.</p>
<hr>
<p>The idea of using Reinforcement learning to derive computationally rational strategies to solve a Visual Search task was introduced in this work. <a href="http://users.comnet.aalto.fi/oulasvir/pubs/pn1895-pcs-pdf.pdf">Chen X.,Bailly G., Brumby D., Oulasvirta A.,Howes A.<strong>The Emergence of Interactive Behaviour:A Model of Rational Menu Search</strong>,2015</a>. The authors investigate the effect of Menu organization <b>(Alphabetic,Semantic, Unordered)</b> and Menu length on the <em>emergent</em> optimal strategy and compare the model predictions with empirical findings.</p>
<p>Our work on this model is a derivative of the aforementioned work in the following ways:</p>
<ul>
<li>
<p>The model makes the same hypothesis of <u>Optimal control</u> that the authors investigated where the task of Menu Search is rationally adapted to Ecological structure of interaction(what agent observes the environment), Mechanism(how agent observes the environment: Perceptual limits,peripheral vision) and Utility( Maximizing speed and accuracy).</p>
</li>
<li>
<p>The model uses the same structure for Reward and Peripheral vision reported in the work where at each step the Time duration to take that step (fixation,saccade duration) is appended as a negative reward. Upon solving the task, the agent is highly rewarded or punished upon failure.</p>
</li>
</ul>
<p>Our model is differentiated in the following ways:</p>
<ul>
<li>The observation model or the Ecology structure uses a <strong>Partially observed model</strong> where the user observes the <u>Semantic and Shape relevance</u> of the fixated item in the Menu and then samples the probability of it being a target from its internal belief distribution.</li>
</ul>
<p><strong>This allows us to parameterize the internal cognitive distribution of the User and can be used to model the very different behaviours observed in Expert vs Novice user for the same task.</strong></p>
<blockquote>
<p>An Expert user would have a belief distribution closer to the true distribution from which these relevances were sampled. The KL divergence between these two distributions could be used as a parameter to model User Expertise.</p>
</blockquote>
<ul>
<li>Therefore now we can solve the PO-MDP as a continuous state MDP where the state is the user's current belief of the Target being present at that location in the Menu along with an additional belief of Target being absent.</li>
</ul>
<p>With the state space now being continuous and hence having practically infinite states a tabular Q learning method such as the one used in the above paper wouldn't work without some sort of Function approximation. We used a <strong>Gaussian Process</strong> method <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6420&amp;rep=rep1&amp;type=pdf">GP-SARSA</a> to approximate the Q function and make use of the covariance matrix to aid the agent's exploration strategy.</p>
<p>The advantage of using such Bayesian methods over currently popular Deep learning/Gradient based methods is that it allows us to encode the prior information from a HCI research perspective in a structured way. This is essential when we are trying to form a simulator model whose eventual aim is to be able to explain empirical findings.</p>
<p>For a complete study of the RL learner Gaussian Process-SARSA there would be another post in the &quot;Reproducibility in Machine Learning&quot; series where I would reproduce the experiments of the original work on a Maze environment.</p>
<hr>
<h3 id="modeloverview">Model Overview</h3>
<p><img src="http://localhost:2368/content/images/2019/03/pomdp_menu.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p>The two versions of the GP-SARSA learner were implemented with sparse and non sparse dictionary. In the sparsified version, a threshold (hyperparameter) determines which state transitions or steps would be used as data points to fit our Gaussian prior. This reduces the execution time of the algorithm from O(t<sup>3</sup>) to O(tm<sup>2</sup>) where t=total transitions and m=data points in dictionary. The ratio of 'm' to 't' is controlled by threshold parameter.</p>
<p>To get a starting point regarding the setting of this threshold i plotted the distribution of the values that this threshold controls(delta). <img src="http://localhost:2368/content/images/2019/03/threshold.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p>With experiments, i observed that a value of 0.5 works best. A higher value excludes a lot of potentially important transitions and a lower value has repetitive transitions with not much entropy.</p>
<h3 id="policy">Policy</h3>
<p>The learner was trained with the following action selection policies:</p>
<ul>
<li><strong>Epsilon-Greedy:</strong> Standard policy used in RL where value of Epsilon controls the exploitation-exploration trade-off. High value of epsilon leads to more exploration.</li>
<li><strong>Active learning</strong> based on Covariance: Here we use the covariance matrix of the Q function Gaussian Process and explore the actions which have currently high uncertainty in their Q values(Expected payoff). Thus we would take,
<ul>
<li>Action with highest mean with 1-Eps probability</li>
<li>Action with highest uncertainty with Epsilon prob.</li>
</ul>
</li>
</ul>
<p>Since the covariance of these State-Action values update as the agent learns, its called an Active learning approach.</p>
<ul>
<li><strong>Stochastic Learning :</strong> Here a parameter called <strong>Covariance Scale</strong> is used which controls the spread of the Q value distribution. The action selection rule is defined as:<br>
<b>Action(s) = argmax<sub>a</sub>{Q'(s, a):a ∈ Ao}</b></li>
</ul>
<p>where,Q'(s,a) denotes a <strong>sample</strong>: Q'(s, a) ∼ N (Q(s, a),η<sup>2</sup>cov((s, a),(s, a)))</p>
<p>and η<sup>2</sup>=Covariance Scale</p>
<p>As iterated in the original literature on GP-SARSA, this policy provides faster convergence but sometimes leads to a sub-optimal policy. This could be explained from the fact that the way this policy handles exploration is by randomly trying Actions with overlapping distribution. In our case the distribution of Click/Quit Actions varies significantly from Fixation Actions and hence this policy inhibits exploring those actions in initial training and thus leads to a policy where a lot of fixations are done before selection.</p>
<p>With our experiments we observed that <u>Active learning</u> led to a better policy.</p>
<hr>
<h3 id="experiments">Experiments</h3>
<p>In the aforementioned work, authors conducted experiments to measure <strong>Selection time</strong> in the Visual Search task to measure how a learner trained on one type of Menu performs across other menus.(on which it was not trained). The Selection time is made up of the time to fixate on an item and saccade duration. Their model corroborated what they observed in empirical data that Alphabetic menu layout is the most efficient in terms of selection time. In the results below, a comparison between Semantic and Unordered layout is done(Alphabetic to follow). The policy is trained on Semantically organised Menu items and the agent's selection time and Gaze proportion is evaluated on Semantic and Unordered Menus.</p>
<p><strong>Selection Time:</strong><br>
We compare the Selection Times of our learner with the results in the above work.</p>
<p><img src="http://localhost:2368/content/images/2019/03/Screenshot-from-2019-01-08-19-30-23.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<center> Fig.5 Search Duration with 95% CI, Chen et.al,2015</center>
<p>The above plot shows the Selection Time for the visual search task in a 10 item Menu. The agent's performance was recorded before and after training on Alphabetic, Semantic and Unordered Menus. For semantically trained learners, the mean selection time is 2000 ms and <u>avg steps ~ 4.6 </u>(~ 437ms per fixation) before making a selection.</p>
<p>We demonstrate a similar plot for the learner's performance before and after training where it's trained on a Semantically organised Menu and its selection time is compared to its performance on an Unordered Menu(more comprehensive evaluation across menus to follow). For our experiments we used a 8 item Menu grouped in 2 and it took on an <u> avg~ 2.8 steps to successful selection</u> in a Semantic Menu.</p>
<p><img src="http://localhost:2368/content/images/2019/03/selection_time.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p><strong>Gaze Proportion</strong></p>
<p>Analyzing the gaze distribution of the user when it's interacting with an Interface is a key to building personalised adaptive UIs. We looked in <a href="https://aasheeshsingh.me/visualsearch1/">Part 1</a> of this series how UI designers are interested in knowing about user's behavior(through mouse movements or gaze) when interacting with their product and thus Gaze Analytics as a Service has become an essential tool for them.(<a href="https://heatmap.com/">Heat Map Inc</a> sells such analytics services as Javascript wrappers)</p>
<p>Continuing in similar fashion ,we take a look at the <strong>Gaze proportions</strong> on the Target item while navigating the Menu, first from the <a href="http://users.comnet.aalto.fi/oulasvir/pubs/pn1895-pcs-pdf.pdf">Chen et.al,2015</a> and then our experiments.</p>
<p><img src="http://localhost:2368/content/images/2019/03/1.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p>Down below, are the plots for <strong>Gaze proportions</strong> in the learned policies trained on Semantic Menus compared with Unordered Menu and Initial untrained learner.</p>
<p><img src="http://localhost:2368/content/images/2019/03/Gaze_distribution-1.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<p>Here we can see that an untrained agent, distributes the gaze evenly among all target items while the trained policy has learnt to focus on specific items in the menu while looking for target.</p>
<p>The reason for such high gaze proportion when the Target is at 3(0 indexed x axis) is because the policy learnt to fixate on Item 3 as the first action to be taken for each episode, <u>as it allows the agent to encode the semantic relevances of both groups in the menu through peripheral vision</u> (encode info of one item above and down from fixation location) and update the belief state.</p>
<p><img src="http://localhost:2368/content/images/2019/03/gaze2.png" alt="Part 2 : Cognitive Models for Visual Search"></p>
<center> Fig: Gaze distribution over Menu types </center>
<hr>
<h3 id="conclusion">Conclusion</h3>
<p>The work in Chen et. al,2015 assumes a <u>fully observable</u> and <u>deterministic</u> Ecology structure where the user knows exactly the relevances of the menu items. This is what our model would formulate as Expert behaviour in terms of observation mechanism. The inclusion of a internal belief distribution whose divergence from this expert true distribution could be used to model the &quot;Familiarity&quot; or &quot;<strong>proficiency</strong>&quot; of the user in this task.</p>
<p>A user who shows a  different mental ability(people with special needs) or cognitive load while doing such tasks would have a different observed behaviour(in terms of selection time/utility) and thus may not fit this model. What our model attempts is to <u>encode such Proficiency as a parameter</u> and also makes it easy to introduce other perceptual signals(apart from semantic and shape) without changing the learner or the environment structure.</p>
<p>In part 1 of this series I have written about how personalization in Streaming,E-commerce and Advertisements has helped us build better products. I believe Interfaces that adapt and change according to the cognitive skills of the user are the next step to building better software.</p>
<p>The other features of this model are <u>scalablity</u> and dealing with observation noise. A tabular Q learner such as one used in the paper suffers from curse of dimensionality as we scale the state-action space, while this continuous model would be better suited to encode further perceptual information and corresponding noise.</p>
<hr>
<blockquote>
<h4 id="inpart3ofthishciserieswewouldinvestigatehowlikelihoodfreeinferencemethodsapproximatebayesiancomputationcouldbeusedtodeduceparametersofthismodelinainverselearningtypeparadigm">In part 3 of this HCI series we would investigate how Likelihood-free inference methods(Approximate Bayesian Computation) could be used to deduce parameters of this model in a inverse learning type paradigm.</h4>
</blockquote>
]]></content:encoded></item><item><title><![CDATA[ML Reproducibility]]></title><description><![CDATA[This blog documents my experience trying to reproduce a peer reviewed publication every 3 weeks. The goal is to form a "Experiment-zoo" to reproduce results as stated in the published research. I would also implement unpublished code papers alongside.  ]]></description><link>http://localhost:2368/private-sites/</link><guid isPermaLink="false">5c6e706aa3007c27278140b7</guid><category><![CDATA[Introduction]]></category><dc:creator><![CDATA[Aasheesh Singh]]></dc:creator><pubDate>Sun, 30 Dec 2018 09:51:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2019/01/Paper1-20repeated-20attempts.width-600_7UiCNvo.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2019/01/Paper1-20repeated-20attempts.width-600_7UiCNvo.jpg" alt="ML Reproducibility"><p>This blog documents my experience trying to reproduce a peer reviewed publication every <strong>3 weeks</strong>. The goal is to form a &quot;Experiment-zoo&quot; to reproduce results as stated in the published research along with implementing the models in case of unpublished code.</p>
]]></content:encoded></item></channel></rss>